<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


### [2] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [3] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [4] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


### [5] [MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference](https://arxiv.org/abs/2511.05321)
*Maximilian Kirschner,Konstantin Dudzik,Ben Krusekamp,JÃ¼rgen Becker*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-time systems, particularly those used in domains like automated driving,
are increasingly adopting neural networks. From this trend arises the need for
high-performance hardware exhibiting predictable timing behavior. While
state-of-the-art real-time hardware often suffers from limited memory and
compute resources, modern AI accelerators typically lack the crucial
predictability due to memory interference.
  We present a new hardware architecture to bridge this gap between performance
and predictability. The architecture features a multi-core vector processor
with predictable cores, each equipped with local scratchpad memories. A central
management core orchestrates access to shared external memory following a
statically determined schedule.
  To evaluate the proposed hardware architecture, we analyze different variants
of our parameterized design. We compare these variants to a baseline
architecture consisting of a single-core vector processor with large vector
registers. We find that configurations with a larger number of smaller cores
achieve better performance due to increased effective memory bandwidth and
higher clock frequencies. Crucially for real-time systems, execution time
fluctuation remains very low, demonstrating the platform's time predictability.

</details>
