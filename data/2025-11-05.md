<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 针对分解式AI GPU中的NUMA瓶颈，提出了Swizzled Head-first Mapping调度策略，在AMD MI300X上实现性能提升50%，L2缓存命中率达80-97%


<details>
  <summary>Details</summary>
Motivation: 分解式AI GPU设计中出现的非统一内存访问(NUMA)问题已成为大规模注意力计算的关键瓶颈，传统GPU调度策略无法有效处理跨计算区域的内存延迟差异

Method: 提出Swizzled Head-first Mapping空间感知调度策略，将注意力头与GPU NUMA域对齐，利用芯片内缓存重用

Result: 在AMD MI300X架构上，相比传统调度技术，性能提升高达50%，L2缓存命中率稳定在80-97%

Conclusion: NUMA感知调度对下一代分解式GPU实现全效运行至关重要，为可扩展AI训练和推理提供了前进路径

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [2] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在CGLA上实现了Whisper核心计算内核，并通过FPGA原型评估和28nm ASIC性能预测，证明了其在能效方面优于CPU和GPU，为功耗受限的边缘设备提供了可持续的ASR解决方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在自动语音识别任务中的兴起带来了严重的能耗挑战，ASIC虽然高效但缺乏算法适应性，需要寻找兼顾能效和可编程性的解决方案。

Method: 在IMAX CGLA加速器上实现Whisper核心计算内核，采用硬件/软件协同设计方法，通过FPGA原型进行评估，并预测28nm ASIC的性能。

Result: 投影的ASIC在Q8_0模型上比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍。

Conclusion: CGLA是功耗受限边缘设备上实现可持续ASR的有前景平台，在能效方面显著优于传统CPU和GPU方案。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [3] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus是一个三阶段框架，通过聚焦LLM推理于关键决策点来增强Verilog代码生成功能正确性，在VerilogEval-Human基准测试中显著提高了pass@1正确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自一致性或仿真反馈选择最佳候选，但未能将LLM推理聚焦于设计中最具信息量的部分，导致功能正确性验证存在挑战。

Method: 三阶段框架：预排序阶段生成代码候选并过滤；排序阶段仿真测试并聚类识别一致输出；后排序精炼阶段挖掘不一致性并调用推理增强LLM提示进行候选精炼。

Result: 在VerilogEval-Human基准测试中，VFocus显著提高了多个推理LLM的pass@1正确率。

Conclusion: VFocus通过聚焦LLM推理于关键决策点，有效提升了复杂硬件设计任务中Verilog生成的功能正确性。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [4] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 在SoC FPGA上实现基于DPU的多线程人脸表情识别系统，使用DenseBox进行人脸检测和CNN进行表情识别，相比传统方法提高了准确率和资源利用率，达到25FPS系统吞吐量和2.4倍能效比


<details>
  <summary>Details</summary>
Motivation: 解决传统Haar Cascade人脸检测器在侧脸和变光照条件下准确率低的问题，同时避免为第二个DNN推理添加专用加速器，实现FPGA资源的高效利用

Method: 使用DPU（脉动阵列型通用CNN加速器）同时运行DenseBox人脸检测和CNN表情识别两个DNN推理，开发多线程技术提高DPU利用率和系统吞吐量

Result: 系统整体吞吐量达到25FPS，单位功耗吞吐量提升2.4倍，实现了在FPGA上的高效人脸表情识别

Conclusion: 基于DPU的多线程方法能够在保持小电路规模的同时有效利用FPGA资源，为嵌入式系统提供高性能的人脸表情识别解决方案

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [5] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了基于数字递归算法的posit除法单元，首次在该领域实现radix-4数字递归技术，通过硬件优化显著降低能耗和迭代次数。


<details>
  <summary>Details</summary>
Motivation: Posit算术作为IEEE 754浮点表示的替代方案，具有更高的精度和动态范围，但其除法操作因硬件复杂性而面临挑战。

Method: 采用数字递归算法，结合冗余算术、在线商转换和操作数缩放等硬件优化技术，实现radix-4数字递归的posit除法单元。

Result: 综合评估显示性能显著提升：能耗降低80%以上（仅需小面积开销），迭代次数大幅减少。

Conclusion: 所提出的算法优化能有效提升基于posit的算术单元效率，为posit系统提供高效的除法解决方案。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [6] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在IMAX3 CGRA加速器上实现了stable-diffusion.cpp图像生成框架的核心计算内核，并进行了深入评估。通过FPGA原型测试和ASIC实现潜力分析，证明了该通用架构在性能和能效方面的优势。


<details>
  <summary>Details</summary>
Motivation: 评估IMAX3作为通用计算平台在执行高要求的图像生成工作负载时的能力，为下一代AI专用CGLA加速器的开发提供基础。

Method: 在IMAX3 CGRA加速器上实现stable-diffusion.cpp的核心计算内核，通过FPGA原型建立性能基线，并预测其ASIC实现的潜力。

Result: 尽管采用通用架构，IMAX3在性能和能效方面表现出色，特别是在预期的ASIC形式下更具优势。

Conclusion: 这项工作为未来IMAX架构设计提供了具体指导，并为开发下一代AI专用CGLA加速器奠定了基础，有助于实现能效高、设备端的多模态AI平台。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>
