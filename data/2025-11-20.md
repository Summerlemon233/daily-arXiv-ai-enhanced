<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU是一个硬件-软件协同设计的系统，通过编译器优化和硬件支持来改善内存密集型应用在分解内存系统中的性能，显著提升了协程效率。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用面临分解内存系统带来的内存延迟挑战，现有协程方法在平衡延迟隐藏效率和运行时开销方面存在困难。

Method: 采用硬件-软件协同设计，包括编译器过程优化协程代码生成、最小化上下文和合并请求，以及硬件支持的解耦内存操作，增强异步内存单元并引入内存引导分支预测机制。

Result: 在FPGA模拟的分解内存系统中，CoroAMU编译器比最先进的协程方法快1.51倍，结合优化硬件后在200ns和800ns延迟下分别实现3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过硬件-软件协同设计有效解决了协程在分解内存系统中的性能瓶颈，显著提升了内存密集型应用的执行效率。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [2] [Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism](https://arxiv.org/abs/2511.15397)
*Cong Wang,Zexin Fu,Jiayi Huang,Shanshi Huang*

Main category: cs.AR

TL;DR: Hemlet是一个异构CIM小芯片系统，旨在通过集成模拟CIM、数字CIM和中间数据处理小芯片来加速视觉Transformer，解决单片CIM设计的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在性能提升的同时带来巨大的内存和计算资源需求，单片CIM设计面临可扩展性限制，而传统小芯片设计又存在通信开销大的问题。

Method: 设计异构CIM小芯片系统，集成模拟CIM、数字CIM和中间数据处理小芯片，通过灵活的资源配置和通信优化来提高吞吐量。

Result: Hemlet系统实现了更高的可扩展性和吞吐量，同时减少了通信开销。

Conclusion: 异构CIM小芯片系统是解决视觉Transformer硬件部署挑战的有效方法，能够在保持性能的同时提高能效和可扩展性。

Abstract: Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.
  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove

</details>


### [3] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，解决PIM设备与主机处理器之间数据布局不匹配的问题，在各种ML内核和LLM推理中实现显著加速。


<details>
  <summary>Details</summary>
Motivation: PIM设备与主机处理器需要不同的数据布局，导致数据重排成为ML内核执行中的主要性能瓶颈，而现有编译方法缺乏对数据重排和计算代码的联合优化支持。

Method: 设计DCC编译器，集成多层PIM抽象，将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并使用快速准确的性能预测模型选择最优配置。

Result: 在HBM-PIM上实现最高7.68倍加速（平均2.7倍），在AttAcc PIM后端实现最高13.17倍加速（平均5.75倍）；在LLM推理中，GPT-3和LLaMA-2最高加速7.71倍（平均4.88倍）。

Conclusion: DCC证明了数据重排和计算代码联合优化的必要性，通过统一的调优过程显著提升了PIM系统上ML内核的性能，为支持多样化PIM后端提供了有效解决方案。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [4] [Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference](https://arxiv.org/abs/2511.15505)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 本文提出了一种基于指令的FPGA多处理器协调架构，用于加速DNN推理，支持可编程同步、灵活模型划分和多种并行策略，实验显示计算效率高达98%，吞吐量提升2.7倍。


<details>
  <summary>Details</summary>
Motivation: 为FPGA多处理器系统开发高效的DNN推理加速架构，解决多处理器协调和同步问题，支持灵活的模型划分和并行策略。

Method: 采用指令控制器和点对点指令同步单元，将指令分为加载、计算和存储功能组，通过编译框架将DNN模型转换为可执行指令程序。

Result: 在ResNet-50上的实验显示计算效率高达98%，吞吐量效率提升2.7倍，支持单批次和多批次性能的动态权衡。

Conclusion: 该架构实现了高效的FPGA多处理器DNN推理加速，具有高计算效率、吞吐量提升和灵活的并行策略支持。

Abstract: This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.

</details>


### [5] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 提出了基于开源Chiplet的RISC-V系统路线图，从已流片的双Chiplet系统扩展到四Chiplet架构，旨在缩小与专有设计的性能差距，并探索EDA工具和物理设计套件的开源化。


<details>
  <summary>Details</summary>
Motivation: 缩小开源RISC-V系统与专有设计在高性能计算和人工智能领域的性能差距，推动从逻辑核心到EDA工具链的全面开源。

Method: 采用渐进式路线：从Occamy（12nm双Chiplet）到Ramora（网状NoC双Chiplet），再到Ogopogo（7nm四Chiplet），同时探索EDA、PDK和PHY接口的开源化。

Result: Occamy成为首个开源硅验证的双Chiplet RISC-V众核系统，Ogopogo概念架构达到业界领先的计算密度。

Conclusion: 基于Chiplet的开源RISC-V系统能够实现高性能，未来需要将开源理念扩展到EDA工具链和物理设计套件等更广泛领域。

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>
